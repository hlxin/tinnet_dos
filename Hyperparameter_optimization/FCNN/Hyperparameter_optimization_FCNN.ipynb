{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bff4d2d",
   "metadata": {},
   "source": [
    "Hyperparameter optimization for FCNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad512b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading modules\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from copy import deepcopy\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from ase.db import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d434326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "\n",
    "def get_train_val_test_loader(dataset,\n",
    "                              idx_validation=0,\n",
    "                              idx_test=None,\n",
    "                              collate_fn=default_collate,\n",
    "                              batch_size=64,\n",
    "                              num_workers=0,\n",
    "                              pin_memory=False,\n",
    "                              random_seed=None):\n",
    "    \n",
    "    indices = np.arange(len(dataset))[:-38]\n",
    "    tmp = np.arange(len(dataset))[-38:] # Last 38 images are pure metals\n",
    "    \n",
    "    if random_seed:\n",
    "        random.Random(random_seed).shuffle(indices)\n",
    "    else:\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    kfold = np.array_split(indices,10)\n",
    "    \n",
    "    kfold_val = deepcopy(kfold[idx_validation])\n",
    "    \n",
    "    try:\n",
    "        kfold_test = deepcopy(kfold[idx_test])\n",
    "    except:\n",
    "        kfold_test = []\n",
    "    \n",
    "    kfold_train = deepcopy([kfold[i]\n",
    "                            for i in range(0,10)\n",
    "                            if i != idx_validation and i != idx_test])\n",
    "    \n",
    "    kfold_train = np.array([item for sl in kfold_train for item in sl])\n",
    "    \n",
    "    kfold_train = np.concatenate((kfold_train,tmp))\n",
    "    \n",
    "    if random_seed:\n",
    "        random.Random(random_seed).shuffle(kfold_train)\n",
    "    else:\n",
    "        random.shuffle(kfold_train)\n",
    "    \n",
    "    val_sampler = SubsetRandomSampler(deepcopy(kfold_val))\n",
    "    test_sampler = SubsetRandomSampler(deepcopy(kfold_test))\n",
    "    train_sampler = SubsetRandomSampler(deepcopy(kfold_train))\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                              sampler=train_sampler,\n",
    "                              num_workers=num_workers,\n",
    "                              collate_fn=collate_fn,\n",
    "                              pin_memory=pin_memory)\n",
    "    \n",
    "    val_loader = DataLoader(dataset, batch_size=4096,\n",
    "                            sampler=val_sampler,\n",
    "                            num_workers=num_workers,\n",
    "                            collate_fn=collate_fn,\n",
    "                            pin_memory=pin_memory)\n",
    "    \n",
    "    test_loader = DataLoader(dataset, batch_size=4096,\n",
    "                             sampler=test_sampler,\n",
    "                             num_workers=num_workers,\n",
    "                             collate_fn=collate_fn,\n",
    "                             pin_memory=pin_memory)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_feature, n_h, h_fea_len, n_output):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc_in = nn.Linear(n_feature, h_fea_len)\n",
    "        self.fc_in_softplus = nn.Softplus()\n",
    "        if n_h > 1:\n",
    "            self.fcs = nn.ModuleList([nn.Linear(h_fea_len, h_fea_len)\n",
    "                                      for _ in range(n_h-1)])\n",
    "            self.softpluses = nn.ModuleList([nn.Softplus()\n",
    "                                             for _ in range(n_h-1)])\n",
    "        self.fc_out = nn.Linear(h_fea_len, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        crys_fea = self.fc_in(x)\n",
    "        crys_fea = self.fc_in_softplus(crys_fea)\n",
    "        \n",
    "        if hasattr(self, 'fcs') and hasattr(self, 'softpluses'):\n",
    "            for fc, softplus in zip(self.fcs, self.softpluses):\n",
    "                crys_fea = softplus(fc(crys_fea))\n",
    "        \n",
    "        out = self.fc_out(crys_fea)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "class TrainFCNN(tune.Trainable):\n",
    "    def _setup(self, config):\n",
    "        \n",
    "        self.lr = config.get('lr', 0.01)\n",
    "        self.h_fea_len = int(config.get('h_fea_len', 128))\n",
    "        self.n_h = int(config.get('n_h', 1))\n",
    "        \n",
    "    def _train(self):\n",
    "        \n",
    "        random_seed = 1234    # reproducible\n",
    "        batch_size = 64\n",
    "        num_workers = 0\n",
    "        weight_decay = 0.0001\n",
    "        \n",
    "        collate_fn = default_collate\n",
    "        \n",
    "        best_val_loss_mae = 1e10\n",
    "        best_val_loss_mse = 1e10\n",
    "        best_test_loss_mae = 1e10\n",
    "        best_test_loss_mse = 1e10\n",
    "        \n",
    "        best_counter = 0\n",
    "        \n",
    "        db = connect('../Database.db')\n",
    "        \n",
    "        d_cen = np.array([r['data']['d_cen'] for r in db.select()])\n",
    "        full_width = np.array([r['data']['full_width'] for r in db.select()])\n",
    "        target = np.stack((d_cen,full_width)).T\n",
    "\n",
    "        v2ds = np.array([r['data']['tabulated_v2ds'] for r in db.select()])\n",
    "        v2dd = np.array([r['data']['tabulated_v2dd'] for r in db.select()])\n",
    "        mulliken = np.array([r['data']['tabulated_mulliken'] for r in db.select()])\n",
    "        d_cen_inf = np.array([r['data']['tabulated_d_cen_inf'] for r in db.select()])\n",
    "        full_width_inf = np.array([r['data']['tabulated_full_width_inf'] for r in db.select()])\n",
    "        fea = np.stack((np.sum((v2ds + v2dd), axis=1), mulliken, d_cen_inf, full_width_inf**2.0/12.0)).T\n",
    "        \n",
    "        target = Variable(torch.Tensor(target))\n",
    "        fea = Variable(torch.Tensor(fea))\n",
    "        \n",
    "        name_images = np.arange(len(fea))\n",
    "        \n",
    "        dataset = [(torch.Tensor(fea[i]),\n",
    "                    name_images[i])\n",
    "                   for i in range(len(fea))]\n",
    "        \n",
    "        train_loader, val_loader, test_loader =\\\n",
    "            get_train_val_test_loader(dataset=dataset,\n",
    "                                      collate_fn=collate_fn,\n",
    "                                      batch_size=batch_size,\n",
    "                                      idx_validation=0,\n",
    "                                      idx_test=1,\n",
    "                                      num_workers=num_workers,\n",
    "                                      pin_memory=torch.cuda.is_available(),\n",
    "                                      random_seed=random_seed)\n",
    "        \n",
    "        self.net = Net(n_feature=fea.shape[-1],\n",
    "                       n_h=self.n_h,\n",
    "                       h_fea_len=self.h_fea_len,\n",
    "                       n_output=2).cuda()\n",
    "        optimizer = torch.optim.AdamW(self.net.parameters(),\n",
    "                                      lr=self.lr,\n",
    "                                      weight_decay=weight_decay)\n",
    "        loss_func = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(100000):\n",
    "            \n",
    "            # switch to train mode\n",
    "            self.net.train()\n",
    "            \n",
    "            for i, (input, batch_cif_ids) in enumerate(train_loader):\n",
    "                prediction = self.net(input.cuda(non_blocking=True))\n",
    "                # loss must be (1. nn output, 2. target)\n",
    "                loss = loss_func(prediction, target[batch_cif_ids].cuda(non_blocking=True))*prediction.shape[-1]\n",
    "                optimizer.zero_grad()   # clear gradients for next train\n",
    "                loss.backward()         # backpropagation, compute gradients\n",
    "                optimizer.step()        # apply gradients\n",
    "            \n",
    "            # switch to evaluate mode\n",
    "            self.net.eval()\n",
    "            \n",
    "            for i, (input, batch_cif_ids) in enumerate(val_loader):\n",
    "                prediction = self.net(input.cuda(non_blocking=True))\n",
    "                val_loss_mae = torch.mean(torch.abs(target[batch_cif_ids].cuda(non_blocking=True) - prediction))*prediction.shape[-1]\n",
    "                val_loss_mse = loss_func(prediction, target[batch_cif_ids].cuda(non_blocking=True))*prediction.shape[-1]\n",
    "            \n",
    "            for i, (input, batch_cif_ids) in enumerate(test_loader):\n",
    "                prediction = self.net(input.cuda(non_blocking=True))\n",
    "                test_loss_mae = torch.mean(torch.abs(target[batch_cif_ids].cuda(non_blocking=True) - prediction))*prediction.shape[-1]\n",
    "                test_loss_mse = loss_func(prediction, target[batch_cif_ids].cuda(non_blocking=True))*prediction.shape[-1]\n",
    "            \n",
    "            best_counter += 1\n",
    "            \n",
    "            if best_val_loss_mse > val_loss_mse:\n",
    "                best_val_loss_mae = val_loss_mae\n",
    "                best_val_loss_mse = val_loss_mse\n",
    "                best_test_loss_mae = test_loss_mae\n",
    "                best_test_loss_mse = test_loss_mse\n",
    "                \n",
    "                best_counter = 0\n",
    "            \n",
    "            if best_counter >= 50: # Exit due to converged\n",
    "                final_ans_val_mae = best_val_loss_mae.detach().cpu().numpy()\n",
    "                final_ans_val_mse = best_val_loss_mse.detach().cpu().numpy()\n",
    "                final_ans_test_mae = best_test_loss_mae.detach().cpu().numpy()\n",
    "                final_ans_test_mse = best_test_loss_mse.detach().cpu().numpy()\n",
    "                break\n",
    "            \n",
    "            if test_loss_mse != test_loss_mse: # Exit due to NaN\n",
    "                final_ans_val_mae = best_val_loss_mae.detach().cpu().numpy()\n",
    "                final_ans_val_mse = best_val_loss_mse.detach().cpu().numpy()\n",
    "                final_ans_test_mae = best_test_loss_mae.detach().cpu().numpy()\n",
    "                final_ans_test_mse = best_test_loss_mse.detach().cpu().numpy()\n",
    "                break\n",
    "        \n",
    "        np.savetxt(path + 'final_ans_val_mae_'\n",
    "                   + str(self.lr)\n",
    "                   + '_'\n",
    "                   + str(self.h_fea_len)\n",
    "                   + '_'\n",
    "                   + str(self.n_h)\n",
    "                   + '.txt', [final_ans_val_mae])\n",
    "        \n",
    "        np.savetxt(path + 'final_ans_val_mse_'\n",
    "                   + str(self.lr)\n",
    "                   + '_'\n",
    "                   + str(self.h_fea_len)\n",
    "                   + '_'\n",
    "                   + str(self.n_h)\n",
    "                   + '.txt', [final_ans_val_mse])\n",
    "        \n",
    "        np.savetxt(path + 'final_ans_test_mae_'\n",
    "                   + str(self.lr)\n",
    "                   + '_'\n",
    "                   + str(self.h_fea_len)\n",
    "                   + '_'\n",
    "                   + str(self.n_h)\n",
    "                   + '.txt', [final_ans_test_mae])\n",
    "        \n",
    "        np.savetxt(path + 'final_ans_test_mse_'\n",
    "                   + str(self.lr)\n",
    "                   + '_'\n",
    "                   + str(self.h_fea_len)\n",
    "                   + '_'\n",
    "                   + str(self.n_h)\n",
    "                   + '.txt', [final_ans_test_mse])\n",
    "        \n",
    "        return {'mean_loss': final_ans_test_mse}\n",
    "    \n",
    "    def _save(self, checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'model.pth')\n",
    "        torch.save(self.net.state_dict(), checkpoint_path)\n",
    "        return checkpoint_path\n",
    "\n",
    "    def _restore(self, checkpoint_path):\n",
    "        self.net.load_state_dict(torch.load(checkpoint_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9bac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    algo = BayesOptSearch(utility_kwargs={\n",
    "        'kind': 'ucb',\n",
    "        'kappa': 2.5,\n",
    "        'xi': 0.0\n",
    "    })\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "    \n",
    "    analysis = tune.run(\n",
    "        TrainFCNN,\n",
    "        name='TrainFCNN',\n",
    "        metric='mean_loss',\n",
    "        mode='min',\n",
    "        search_alg=algo,\n",
    "        scheduler=scheduler,\n",
    "        stop={\n",
    "            'mean_loss': 0.001,\n",
    "            'training_iteration': 20,\n",
    "        },\n",
    "        resources_per_trial={\n",
    "            'cpu': 12,\n",
    "            'gpu': 1\n",
    "        },\n",
    "        num_samples= 500,\n",
    "        checkpoint_at_end=True,\n",
    "        checkpoint_freq=20,\n",
    "        config={\n",
    "            'lr': tune.loguniform(lower=0.0001, upper=0.01, base=10),\n",
    "            'h_fea_len': tune.uniform(lower=16, upper=301),\n",
    "            'n_h': tune.uniform(lower=1, upper=11),\n",
    "        })\n",
    "    \n",
    "    print('Best config is:', analysis.get_best_config(metric='mean_loss',\n",
    "                                                      mode='min'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
